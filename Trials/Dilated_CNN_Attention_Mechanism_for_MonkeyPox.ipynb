{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlC2Bx3-Kozy"
      },
      "outputs": [],
      "source": [
        "# Optimized Dilated CNN with Attention Mechanism for MonkeyPox Detection\n",
        "# Designed for Google Colab with zip folder handling\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XcxJY8t5Yqwh"
      },
      "outputs": [],
      "source": [
        "# Your DatasetManager class \n",
        "class DatasetManager:\n",
        "    \"\"\"Handles dataset extraction, cleaning, and preprocessing\"\"\"\n",
        "    def __init__(self, zip_path, extract_to='./dataset'):\n",
        "        self.zip_path = zip_path\n",
        "        self.extract_to = Path(extract_to)\n",
        "        self.base_path = self.extract_to / 'monkeypox_dataset'\n",
        "        self.classes = ['Chickenpox', 'Cowpox', 'HFMD', 'Measles', 'Monkeypox', 'Normal']\n",
        "        self.splits = ['train', 'val', 'test']\n",
        "\n",
        "    def extract_and_setup(self):\n",
        "        \"\"\"Extract zip file and setup folder structure\"\"\"\n",
        "        print(\"Extracting dataset...\")\n",
        "        if self.extract_to.exists():\n",
        "            shutil.rmtree(self.extract_to)\n",
        "        with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.extract_to)\n",
        "        possible_paths = [\n",
        "            self.extract_to / 'monkeypox_dataset',\n",
        "            self.extract_to / 'dataset' / 'monkeypox_dataset',\n",
        "            list(self.extract_to.glob('*monkeypox*'))[0] if list(self.extract_to.glob('*monkeypox*')) else None\n",
        "        ]\n",
        "        for path in possible_paths:\n",
        "            if path and path.exists():\n",
        "                self.base_path = path\n",
        "                break\n",
        "        print(f\"Dataset extracted to: {self.base_path}\")\n",
        "        return self.merge_healthy_normal()\n",
        "\n",
        "    def merge_healthy_normal(self):\n",
        "        \"\"\"Merge Healthy folder into Normal and remove Healthy\"\"\"\n",
        "        print(\"Merging Healthy folder into Normal...\")\n",
        "        for split in self.splits:\n",
        "            healthy_path = self.base_path / split / 'Healthy'\n",
        "            normal_path = self.base_path / split / 'Normal'\n",
        "            if healthy_path.exists():\n",
        "                normal_path.mkdir(parents=True, exist_ok=True)\n",
        "                for img_file in healthy_path.glob('*'):\n",
        "                    if img_file.is_file() and img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                        shutil.move(str(img_file), str(normal_path))\n",
        "                shutil.rmtree(healthy_path)\n",
        "                print(f\"Merged {split}/Healthy into {split}/Normal\")\n",
        "        return self.clean_dataset()\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        \"\"\"Clean corrupt images and analyze distribution\"\"\"\n",
        "        print(\"Cleaning dataset...\")\n",
        "        counts = {}\n",
        "        corrupt_count = 0\n",
        "        for split in self.splits:\n",
        "            counts[split] = {}\n",
        "            split_path = self.base_path / split\n",
        "            for cls in self.classes:\n",
        "                cls_path = split_path / cls\n",
        "                if not cls_path.exists():\n",
        "                    counts[split][cls] = 0\n",
        "                    continue\n",
        "                valid_images = []\n",
        "                for img_path in cls_path.glob('*'):\n",
        "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                        try:\n",
        "                            with Image.open(img_path) as img:\n",
        "                                img.verify()\n",
        "                            valid_images.append(img_path)\n",
        "                        except Exception:\n",
        "                            img_path.unlink()\n",
        "                            corrupt_count += 1\n",
        "                counts[split][cls] = len(valid_images)\n",
        "        print(f\"Removed {corrupt_count} corrupt images\")\n",
        "        df_counts = pd.DataFrame(counts).T\n",
        "        print(\"\\nDataset distribution:\")\n",
        "        print(df_counts)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        df_counts.plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "        plt.title('Dataset Distribution After Cleaning')\n",
        "        plt.ylabel('Number of Images')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return df_counts\n",
        "\n",
        "# Run DatasetManager\n",
        "zip_path = '/content/drive/MyDrive/monkeypox_dataset.zip'  # Adjust if uploaded locally\n",
        "dataset_manager = DatasetManager(zip_path=zip_path, extract_to='/content/dataset')\n",
        "df_counts = dataset_manager.extract_and_setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKcpPhWDLnkP"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0xU7eM8Nxa8"
      },
      "outputs": [],
      "source": [
        "class DilatedAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dilation_rate=2):\n",
        "        super(DilatedAttentionBlock, self).__init__()\n",
        "        self.dilated_conv = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3, padding=dilation_rate,\n",
        "            dilation=dilation_rate, bias=False\n",
        "        )\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(out_channels, out_channels // 16, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels // 16, out_channels, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.spatial_attention = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dilated_conv(x)\n",
        "        out = self.bn(out)\n",
        "        ca = self.channel_attention(out)\n",
        "        out = out * ca\n",
        "        avg_out = torch.mean(out, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(out, dim=1, keepdim=True)\n",
        "        sa_input = torch.cat([avg_out, max_out], dim=1)\n",
        "        sa = self.spatial_attention(sa_input)\n",
        "        out = out * sa\n",
        "        return self.relu(out)\n",
        "\n",
        "class DilatedAttentionResNet(nn.Module):\n",
        "    def __init__(self, num_classes=6, pretrained=True):\n",
        "        super(DilatedAttentionResNet, self).__init__()\n",
        "        self.backbone = torchvision.models.resnet50(pretrained=pretrained)\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
        "        self.dilated_block1 = DilatedAttentionBlock(2048, 512, dilation_rate=2)\n",
        "        self.dilated_block2 = DilatedAttentionBlock(512, 256, dilation_rate=4)\n",
        "        self.dilated_block3 = DilatedAttentionBlock(256, 128, dilation_rate=8)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        x = self.dilated_block1(features)\n",
        "        x = self.dilated_block2(x)\n",
        "        x = self.dilated_block3(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class AdvancedDataset(Dataset):\n",
        "    def __init__(self, root_path, classes, transform=None, class_specific_aug=False):\n",
        "        self.root_path = Path(root_path)\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
        "        self.transform = transform\n",
        "        self.class_specific_aug = class_specific_aug\n",
        "        self.samples = []\n",
        "        self.class_counts = Counter()\n",
        "        for cls in classes:\n",
        "            cls_path = self.root_path / cls\n",
        "            if cls_path.exists():\n",
        "                for img_path in cls_path.glob('*'):\n",
        "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
        "                        self.samples.append((str(img_path), self.class_to_idx[cls]))\n",
        "                        self.class_counts[cls] += 1\n",
        "        median_count = np.median(list(self.class_counts.values()))\n",
        "        self.minority_classes = {cls for cls, count in self.class_counts.items() if count < median_count}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = np.array(Image.open(img_path).convert('RGB'))\n",
        "        if self.class_specific_aug and self.classes[label] in self.minority_classes:\n",
        "            transform = get_minority_augmentation()\n",
        "        else:\n",
        "            transform = self.transform if self.transform else get_standard_augmentation()\n",
        "        if transform:\n",
        "            if isinstance(transform, A.Compose):\n",
        "                augmented = transform(image=image)\n",
        "                image = augmented['image']\n",
        "            else:\n",
        "                image = Image.fromarray(image)\n",
        "                image = transform(image)\n",
        "        return image, label\n",
        "\n",
        "def get_standard_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomCrop(224, 224),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=15, p=0.5),\n",
        "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, p=0.5),\n",
        "        A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_minority_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Resize(256, 256),\n",
        "        A.RandomCrop(224, 224),\n",
        "        A.HorizontalFlip(p=0.6),\n",
        "        A.Rotate(limit=20, p=0.7),\n",
        "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, p=0.7),\n",
        "        A.ElasticTransform(alpha=50, sigma=5, p=0.5),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    return A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsveluPNNzzW"
      },
      "outputs": [],
      "source": [
        "# ModelTrainer\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, test_loader, class_names, device):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.class_names = class_names\n",
        "        self.device = device\n",
        "        self.setup_training()\n",
        "        self.train_losses = []\n",
        "        self.train_accuracies = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "\n",
        "    def setup_training(self):\n",
        "        all_labels = []\n",
        "        for _, labels in self.train_loader:\n",
        "            all_labels.extend(labels.numpy())\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(all_labels), y=all_labels)\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(self.device))\n",
        "        backbone_params = []\n",
        "        attention_params = []\n",
        "        classifier_params = []\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if 'backbone' in name:\n",
        "                backbone_params.append(param)\n",
        "            elif 'dilated_block' in name:\n",
        "                attention_params.append(param)\n",
        "            else:\n",
        "                classifier_params.append(param)\n",
        "        self.optimizer = torch.optim.AdamW([\n",
        "            {'params': backbone_params, 'lr': 1e-5},\n",
        "            {'params': attention_params, 'lr': 1e-4},\n",
        "            {'params': classifier_params, 'lr': 1e-3}\n",
        "        ], weight_decay=1e-4)\n",
        "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(data)\n",
        "            loss = self.criterion(outputs, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += target.size(0)\n",
        "            correct_predictions += (predicted == target).sum().item()\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f'Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}')\n",
        "        epoch_loss = running_loss / len(self.train_loader)\n",
        "        epoch_acc = 100.0 * correct_predictions / total_samples\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.val_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                outputs = self.model(data)\n",
        "                loss = self.criterion(outputs, target)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_samples += target.size(0)\n",
        "                correct_predictions += (predicted == target).sum().item()\n",
        "        epoch_loss = running_loss / len(self.val_loader)\n",
        "        epoch_acc = 100.0 * correct_predictions / total_samples\n",
        "        return epoch_loss, epoch_acc\n",
        "\n",
        "    def train(self, num_epochs=50, save_best=True):\n",
        "        best_val_acc = 0.0\n",
        "        patience = 10\n",
        "        patience_counter = 0\n",
        "        print(f\"Starting training for {num_epochs} epochs...\")\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "            print(\"-\" * 30)\n",
        "            train_loss, train_acc = self.train_epoch()\n",
        "            val_loss, val_acc = self.validate()\n",
        "            self.scheduler.step()\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.train_accuracies.append(train_acc)\n",
        "            self.val_losses.append(val_loss)\n",
        "            self.val_accuracies.append(val_acc)\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                if save_best:\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': self.model.state_dict(),\n",
        "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                        'val_acc': val_acc,\n",
        "                        'val_loss': val_loss\n",
        "                    }, '/content/drive/MyDrive/best_monkeypox_model.pth')\n",
        "                    print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "        print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "        return best_val_acc\n",
        "\n",
        "    def evaluate_test(self):\n",
        "        self.model.eval()\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        with torch.no_grad():\n",
        "            for data, target in self.test_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                outputs = self.model(data)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                y_true.extend(target.cpu().numpy())\n",
        "                y_pred.extend(predicted.cpu().numpy())\n",
        "        test_acc = accuracy_score(y_true, y_pred)\n",
        "        print(f\"\\nTest Accuracy: {test_acc * 100:.2f}%\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred, target_names=self.class_names))\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=self.class_names, yticklabels=self.class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        return test_acc\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        epochs = range(1, len(self.train_losses) + 1)\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
        "        plt.plot(epochs, self.val_losses, 'r-', label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n",
        "        plt.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU4RdWS_QirW"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(base_path, classes, batch_size=32, num_workers=2):\n",
        "    train_dataset = AdvancedDataset(base_path / 'train', classes, transform=get_standard_augmentation(), class_specific_aug=True)\n",
        "    val_dataset = AdvancedDataset(base_path / 'val', classes, transform=get_validation_augmentation())\n",
        "    test_dataset = AdvancedDataset(base_path / 'test', classes, transform=get_validation_augmentation())\n",
        "    print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "    class_counts = train_dataset.class_counts\n",
        "    labels = [label for _, label in train_dataset.samples]\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "    sample_weights = [class_weights[label] for label in labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader, class_counts\n",
        "\n",
        "def main(zip_path):\n",
        "    print(\"=== Setting up Dataset ===\")\n",
        "    dataset_manager = DatasetManager(zip_path)\n",
        "    df_counts = dataset_manager.extract_and_setup()\n",
        "    classes = dataset_manager.classes\n",
        "    base_path = dataset_manager.base_path\n",
        "    print(f\"Final classes: {classes}\")\n",
        "    print(\"\\n=== Creating Data Loaders ===\")\n",
        "    train_loader, val_loader, test_loader, class_counts = create_data_loaders(base_path, classes, batch_size=32, num_workers=2)\n",
        "    print(\"Class distribution in training set:\")\n",
        "    for cls, count in class_counts.items():\n",
        "        print(f\"{cls}: {count}\")\n",
        "    print(\"\\n=== Initializing Model ===\")\n",
        "    model = DilatedAttentionResNet(num_classes=len(classes), pretrained=True)\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(\"\\n=== Setting up Trainer ===\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    trainer = ModelTrainer(model, train_loader, val_loader, test_loader, classes, device)\n",
        "    print(\"\\n=== Starting Training ===\")\n",
        "    best_val_acc = trainer.train(num_epochs=50, save_best=True)\n",
        "    print(\"\\n=== Plotting Training History ===\")\n",
        "    trainer.plot_training_history()\n",
        "    print(\"\\n=== Evaluating on Test Set ===\")\n",
        "    if os.path.exists('/content/drive/MyDrive/best_monkeypox_model.pth'):\n",
        "        checkpoint = torch.load('/content/drive/MyDrive/best_monkeypox_model.pth', map_location=device)\n",
        "        trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(\"Loaded best model for evaluation\")\n",
        "    test_acc = trainer.evaluate_test()\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "    return trainer, best_val_acc, test_acc\n",
        "\n",
        "# Run the pipeline\n",
        "zip_path = '/content/drive/MyDrive/monkeypox_dataset.zip'  # Adjust if local\n",
        "trainer, best_val_acc, test_acc = main(zip_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
