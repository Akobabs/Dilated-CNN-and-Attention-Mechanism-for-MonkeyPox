{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e69588",
   "metadata": {},
   "source": [
    "# ðŸµ Monkeypox Classification with Dilated-CNN + Attention in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a4590",
   "metadata": {},
   "source": [
    "\n",
    "This notebook implements an end-to-end pipeline for classifying **Monkeypox and other similar skin diseases** using a custom **Dilated CNN** architecture enhanced with a **Channel + Spatial Attention Mechanism**.\n",
    "\n",
    "Key features:\n",
    "- Dataset management and cleaning\n",
    "- Dilated CNN with attention blocks\n",
    "- Advanced data augmentation\n",
    "- Weighted sampling to address class imbalance\n",
    "- Full training/validation/testing loop with early stopping\n",
    "- Colab-friendly paths and instructions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ“¦ Install necessary libraries if not available (Colab)\n",
    "!pip install -q albumentations\n",
    "\n",
    "# âœ… Imports\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8ea3e",
   "metadata": {},
   "source": [
    "## ðŸ“ Dataset Extraction & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78436616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Optimized Dilated CNN with Attention Mechanism for MonkeyPox Detection\n",
    "# Key fixes implemented for common issues\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class DatasetManager:\n",
    "    \"\"\"Handles dataset extraction, cleaning, and preprocessing with better error handling\"\"\"\n",
    "    def __init__(self, zip_path, extract_to='./dataset'):\n",
    "        self.zip_path = zip_path\n",
    "        self.extract_to = Path(extract_to)\n",
    "        self.base_path = None\n",
    "        self.classes = ['Chickenpox', 'Cowpox', 'HFMD', 'Measles', 'Monkeypox', 'Normal']\n",
    "        self.splits = ['train', 'val', 'test']\n",
    "        \n",
    "        # Check if zip file exists\n",
    "        if not os.path.exists(zip_path):\n",
    "            raise FileNotFoundError(f\"Zip file not found at: {zip_path}\")\n",
    "\n",
    "    def extract_and_setup(self):\n",
    "        \"\"\"Extract zip file and setup folder structure with better error handling\"\"\"\n",
    "        print(\"Extracting dataset...\")\n",
    "        \n",
    "        # Clean up existing directory\n",
    "        if self.extract_to.exists():\n",
    "            shutil.rmtree(self.extract_to)\n",
    "        \n",
    "        try:\n",
    "            with zipfile.ZipFile(self.zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.extract_to)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to extract zip file: {e}\")\n",
    "        \n",
    "        # Find the correct dataset path\n",
    "        possible_paths = [\n",
    "            self.extract_to / 'monkeypox_dataset',\n",
    "            self.extract_to / 'dataset' / 'monkeypox_dataset',\n",
    "        ]\n",
    "        \n",
    "        # Also search for any folder containing 'monkeypox'\n",
    "        for item in self.extract_to.rglob('*'):\n",
    "            if item.is_dir() and 'monkeypox' in item.name.lower():\n",
    "                possible_paths.append(item)\n",
    "        \n",
    "        # Find the path that contains the expected structure\n",
    "        for path in possible_paths:\n",
    "            if path and path.exists():\n",
    "                # Check if this path has the expected structure\n",
    "                train_path = path / 'train'\n",
    "                if train_path.exists():\n",
    "                    self.base_path = path\n",
    "                    break\n",
    "        \n",
    "        if self.base_path is None:\n",
    "            # List what was extracted to help debug\n",
    "            print(\"Extracted contents:\")\n",
    "            for item in self.extract_to.rglob('*'):\n",
    "                print(f\"  {item}\")\n",
    "            raise Exception(\"Could not find dataset with expected structure\")\n",
    "        \n",
    "        print(f\"Dataset extracted to: {self.base_path}\")\n",
    "        return self.clean_dataset()\n",
    "\n",
    "    def clean_dataset(self):\n",
    "        \"\"\"Clean corrupt images and analyze distribution\"\"\"\n",
    "        print(\"Cleaning dataset...\")\n",
    "        counts = {}\n",
    "        corrupt_count = 0\n",
    "        \n",
    "        for split in self.splits:\n",
    "            counts[split] = {}\n",
    "            split_path = self.base_path / split\n",
    "            \n",
    "            if not split_path.exists():\n",
    "                print(f\"Warning: Split {split} does not exist\")\n",
    "                for cls in self.classes:\n",
    "                    counts[split][cls] = 0\n",
    "                continue\n",
    "            \n",
    "            for cls in self.classes:\n",
    "                cls_path = split_path / cls\n",
    "                if not cls_path.exists():\n",
    "                    counts[split][cls] = 0\n",
    "                    continue\n",
    "                \n",
    "                valid_images = []\n",
    "                for img_path in cls_path.glob('*'):\n",
    "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                        try:\n",
    "                            with Image.open(img_path) as img:\n",
    "                                img.verify()\n",
    "                            # Re-open for actual use (verify closes the file)\n",
    "                            with Image.open(img_path) as img:\n",
    "                                img.load()\n",
    "                            valid_images.append(img_path)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Removing corrupt image: {img_path} - {e}\")\n",
    "                            try:\n",
    "                                img_path.unlink()\n",
    "                                corrupt_count += 1\n",
    "                            except:\n",
    "                                pass\n",
    "                \n",
    "                counts[split][cls] = len(valid_images)\n",
    "        \n",
    "        print(f\"Removed {corrupt_count} corrupt images\")\n",
    "        \n",
    "        # Create DataFrame and display\n",
    "        df_counts = pd.DataFrame(counts).T\n",
    "        print(\"\\nDataset distribution:\")\n",
    "        print(df_counts)\n",
    "        \n",
    "        # Plot distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        df_counts.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "        plt.title('Dataset Distribution After Cleaning')\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf9ce0",
   "metadata": {},
   "source": [
    "## ðŸ§  Dilated Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedAttentionBlock(nn.Module):\n",
    "    \"\"\"Fixed Dilated Attention Block with proper initialization\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dilation_rate=2):\n",
    "        super(DilatedAttentionBlock, self).__init__()\n",
    "        \n",
    "        # Ensure minimum channels for attention\n",
    "        reduction = max(16, out_channels // 16)\n",
    "        \n",
    "        self.dilated_conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, \n",
    "            padding=dilation_rate, dilation=dilation_rate, bias=False\n",
    "        )\n",
    "        \n",
    "        # Channel attention with proper dimensions\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(out_channels, out_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels // reduction, out_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dilated_conv(x)\n",
    "        out = self.bn(out)\n",
    "        \n",
    "        # Channel attention\n",
    "        ca = self.channel_attention(out)\n",
    "        out = out * ca\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(out, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(out, dim=1, keepdim=True)\n",
    "        sa_input = torch.cat([avg_out, max_out], dim=1)\n",
    "        sa = self.spatial_attention(sa_input)\n",
    "        out = out * sa\n",
    "        \n",
    "        return self.relu(out)\n",
    "\n",
    "class DilatedAttentionResNet(nn.Module):\n",
    "    \"\"\"Fixed ResNet with Dilated Attention - better architecture\"\"\"\n",
    "    def __init__(self, num_classes=6, pretrained=True):\n",
    "        super(DilatedAttentionResNet, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        resnet = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Remove the final avgpool and fc layers\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # Progressive dilated attention blocks\n",
    "        self.dilated_block1 = DilatedAttentionBlock(2048, 512, dilation_rate=2)\n",
    "        self.dilated_block2 = DilatedAttentionBlock(512, 256, dilation_rate=4)\n",
    "        self.dilated_block3 = DilatedAttentionBlock(256, 128, dilation_rate=8)\n",
    "        \n",
    "        # Global pooling and classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize new layers\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        x = self.dilated_block1(features)\n",
    "        x = self.dilated_block2(x)\n",
    "        x = self.dilated_block3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DilatedAttentionResNet(Dataset):\n",
    "    \"\"\"Fixed dataset class with better error handling\"\"\"\n",
    "    def __init__(self, root_path, classes, transform=None, class_specific_aug=False):\n",
    "        self.root_path = Path(root_path)\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.transform = transform\n",
    "        self.class_specific_aug = class_specific_aug\n",
    "        self.samples = []\n",
    "        self.class_counts = Counter()\n",
    "        \n",
    "        # Collect all samples\n",
    "        for cls in classes:\n",
    "            cls_path = self.root_path / cls\n",
    "            if cls_path.exists():\n",
    "                for img_path in cls_path.glob('*'):\n",
    "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                        self.samples.append((str(img_path), self.class_to_idx[cls]))\n",
    "                        self.class_counts[cls] += 1\n",
    "        \n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(f\"No valid images found in {self.root_path}\")\n",
    "        \n",
    "        # Identify minority classes\n",
    "        if len(self.class_counts) > 0:\n",
    "            median_count = np.median(list(self.class_counts.values()))\n",
    "            self.minority_classes = {cls for cls, count in self.class_counts.items() if count < median_count}\n",
    "        else:\n",
    "            self.minority_classes = set()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.class_specific_aug and self.classes[label] in self.minority_classes:\n",
    "            transform = get_minority_augmentation()\n",
    "        else:\n",
    "            transform = self.transform if self.transform else get_standard_augmentation()\n",
    "        \n",
    "        if transform:\n",
    "            try:\n",
    "                if isinstance(transform, A.Compose):\n",
    "                    augmented = transform(image=image)\n",
    "                    image = augmented['image']\n",
    "                else:\n",
    "                    image = Image.fromarray(image)\n",
    "                    image = transform(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying transform: {e}\")\n",
    "                # Fallback to basic tensor conversion\n",
    "                image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def get_standard_augmentation():\n",
    "    \"\"\"Standard augmentation pipeline\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomCrop(224, 224),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_minority_augmentation():\n",
    "    \"\"\"Enhanced augmentation for minority classes\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomCrop(224, 224),\n",
    "        A.HorizontalFlip(p=0.6),\n",
    "        A.Rotate(limit=20, p=0.7),\n",
    "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, p=0.7),\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Validation augmentation (minimal)\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Fixed model trainer with better error handling and learning rate scheduling\"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, class_names, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.class_names = class_names\n",
    "        self.device = device\n",
    "        self.setup_training()\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def setup_training(self):\n",
    "        \"\"\"Setup training components with proper class weighting\"\"\"\n",
    "        # Calculate class weights\n",
    "        all_labels = []\n",
    "        for batch_idx, (_, labels) in enumerate(self.train_loader):\n",
    "            all_labels.extend(labels.numpy())\n",
    "            if batch_idx > 100:  # Sample first 100 batches for efficiency\n",
    "                break\n",
    "        \n",
    "        unique_classes = np.unique(all_labels)\n",
    "        class_weights = compute_class_weight('balanced', classes=unique_classes, y=all_labels)\n",
    "        \n",
    "        # Create weight tensor\n",
    "        weight_tensor = torch.ones(len(self.class_names))\n",
    "        for i, weight in zip(unique_classes, class_weights):\n",
    "            weight_tensor[i] = weight\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(weight=weight_tensor.to(self.device))\n",
    "        \n",
    "        # Setup optimizer with different learning rates\n",
    "        backbone_params = []\n",
    "        attention_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                backbone_params.append(param)\n",
    "            elif 'dilated_block' in name:\n",
    "                attention_params.append(param)\n",
    "            else:\n",
    "                classifier_params.append(param)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 1e-5, 'weight_decay': 1e-4},\n",
    "            {'params': attention_params, 'lr': 1e-4, 'weight_decay': 1e-4},\n",
    "            {'params': classifier_params, 'lr': 1e-3, 'weight_decay': 1e-4}\n",
    "        ])\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=2, eta_min=1e-7\n",
    "        )\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            try:\n",
    "                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, target)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += target.size(0)\n",
    "                correct_predictions += (predicted == target).sum().item()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f'Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader) if len(self.train_loader) > 0 else 0\n",
    "        epoch_acc = 100.0 * correct_predictions / total_samples if total_samples > 0 else 0\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_loader:\n",
    "                try:\n",
    "                    data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
    "                    outputs = self.model(data)\n",
    "                    loss = self.criterion(outputs, target)\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_samples += target.size(0)\n",
    "                    correct_predictions += (predicted == target).sum().item()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader) if len(self.val_loader) > 0 else 0\n",
    "        epoch_acc = 100.0 * correct_predictions / total_samples if total_samples > 0 else 0\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def train(self, num_epochs=50, save_best=True, save_path='/content/best_monkeypox_model.pth'):\n",
    "        \"\"\"Main training loop with early stopping\"\"\"\n",
    "        best_val_acc = 0.0\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            print(f'Learning Rate: {self.optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                \n",
    "                if save_best:\n",
    "                    try:\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'val_acc': val_acc,\n",
    "                            'val_loss': val_loss,\n",
    "                            'class_names': self.class_names\n",
    "                        }, save_path)\n",
    "                        print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving model: {e}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        return best_val_acc\n",
    "\n",
    "    def evaluate_test(self):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                try:\n",
    "                    data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
    "                    outputs = self.model(data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    \n",
    "                    y_true.extend(target.cpu().numpy())\n",
    "                    y_pred.extend(predicted.cpu().numpy())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in test batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if len(y_true) == 0:\n",
    "            print(\"No test data processed successfully\")\n",
    "            return 0.0\n",
    "        \n",
    "        test_acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"\\nTest Accuracy: {test_acc * 100:.2f}%\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=self.class_names, zero_division=0))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return test_acc\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.train_losses:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "        \n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, self.val_losses, 'r-', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n",
    "        plt.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def create_data_loaders(base_path, classes, batch_size=32, num_workers=2):\n",
    "    \"\"\"Create data loaders with error handling\"\"\"\n",
    "    try:\n",
    "        train_dataset = AdvancedDataset(\n",
    "            base_path / 'train', classes, \n",
    "            transform=get_standard_augmentation(), \n",
    "            class_specific_aug=True\n",
    "        )\n",
    "        val_dataset = AdvancedDataset(\n",
    "            base_path / 'val', classes, \n",
    "            transform=get_validation_augmentation()\n",
    "        )\n",
    "        test_dataset = AdvancedDataset(\n",
    "            base_path / 'test', classes, \n",
    "            transform=get_validation_augmentation()\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        \n",
    "        # Create weighted sampler for balanced training\n",
    "        if len(train_dataset) > 0:\n",
    "            class_counts = train_dataset.class_counts\n",
    "            labels = [label for _, label in train_dataset.samples]\n",
    "            \n",
    "            if len(set(labels)) > 1:  # Only if we have multiple classes\n",
    "                class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "                sample_weights = [class_weights[label] for label in labels]\n",
    "                sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "            else:\n",
    "                sampler = None\n",
    "        else:\n",
    "            sampler = None\n",
    "        \n",
    "        # Adjust num_workers based on availability\n",
    "        if num_workers > 0 and not torch.cuda.is_available():\n",
    "            num_workers = min(num_workers, os.cpu_count())\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            sampler=sampler,\n",
    "            shuffle=(sampler is None),\n",
    "            num_workers=num_workers, \n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader, train_dataset.class_counts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data loaders: {e}\")\n",
    "        raise\n",
    "\n",
    "def main(zip_path):\n",
    "    \"\"\"Main execution function with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        # Device setup\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        print(\"=== Setting up Dataset ===\")\n",
    "        dataset_manager = DatasetManager(zip_path)\n",
    "        df_counts = dataset_manager.extract_and_setup()\n",
    "        \n",
    "        classes = dataset_manager.classes\n",
    "        base_path = dataset_manager.base_path\n",
    "        print(f\"Final classes: {classes}\")\n",
    "        \n",
    "        print(\"\\n=== Creating Data Loaders ===\")\n",
    "        train_loader, val_loader, test_loader, class_counts = create_data_loaders(\n",
    "            base_path, classes, batch_size=16, num_workers=2  # Reduced batch size\n",
    "        )\n",
    "        \n",
    "        print(\"Class distribution in training set:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            print(f\"{cls}: {count}\")\n",
    "        \n",
    "        print(\"\\n=== Initializing Model ===\")\n",
    "        model = DilatedAttentionResNet(num_classes=len(classes), pretrained=True)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        print(\"\\n=== Setting up Trainer ===\")\n",
    "        trainer = ModelTrainer(model, train_loader, val_loader, test_loader, classes, device)\n",
    "        \n",
    "        print(\"\\n=== Starting Training ===\")\n",
    "        best_val_acc = trainer.train(num_epochs=30, save_best=True)  # Reduced epochs for testing\n",
    "        \n",
    "        print(\"\\n=== Plotting Training History ===\")\n",
    "        trainer.plot_training_history()\n",
    "        \n",
    "        print(\"\\n=== Evaluating on Test Set ===\")\n",
    "        # Load best model if available\n",
    "        save_path = '/content/best_monkeypox_model.pth'\n",
    "        if os.path.exists(save_path):\n",
    "            try:\n",
    "                checkpoint = torch.load(save_path, map_location=device)\n",
    "                trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(\"Loaded best model for evaluation\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading best model: {e}, using current model\")\n",
    "        \n",
    "        test_acc = trainer.evaluate_test()\n",
    "        \n",
    "        print(\"\\n=== Training Complete ===\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "        \n",
    "        return trainer, best_val_acc, test_acc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, 0.0, 0.0\n",
    "\n",
    "# Example usage - Update these paths according to your setup\n",
    "if __name__ == \"__main__\":\n",
    "    # For Google Colab with Google Drive\n",
    "    zip_path = '/content/drive/MyDrive/monkeypox_dataset.zip'\n",
    "    \n",
    "    # Alternative paths for different setups:\n",
    "    # Local upload: zip_path = '/content/monkeypox_dataset.zip'\n",
    "    # Custom path: zip_path = 'path/to/your/monkeypox_dataset.zip'\n",
    "    \n",
    "    # Check if file exists before running\n",
    "    if os.path.exists(zip_path):\n",
    "        trainer, best_val_acc, test_acc = main(zip_path)\n",
    "    else:\n",
    "        print(f\"Dataset file not found at: {zip_path}\")\n",
    "        print(\"Please upload your dataset or update the path\")\n",
    "        \n",
    "        # Show how to upload in Colab\n",
    "        print(\"\\nTo upload dataset in Colab:\")\n",
    "        print(\"1. Use: from google.colab import files\")\n",
    "        print(\"2. Run: uploaded = files.upload()\")\n",
    "        print(\"3. Update zip_path to match uploaded filename\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc6a6e",
   "metadata": {},
   "source": [
    "## ðŸ§ª Dataset Class & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e15d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDataset(Dataset):\n",
    "    \"\"\"Fixed dataset class with better error handling\"\"\"\n",
    "    def __init__(self, root_path, classes, transform=None, class_specific_aug=False):\n",
    "        self.root_path = Path(root_path)\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        self.transform = transform\n",
    "        self.class_specific_aug = class_specific_aug\n",
    "        self.samples = []\n",
    "        self.class_counts = Counter()\n",
    "        \n",
    "        # Collect all samples\n",
    "        for cls in classes:\n",
    "            cls_path = self.root_path / cls\n",
    "            if cls_path.exists():\n",
    "                for img_path in cls_path.glob('*'):\n",
    "                    if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:\n",
    "                        self.samples.append((str(img_path), self.class_to_idx[cls]))\n",
    "                        self.class_counts[cls] += 1\n",
    "        \n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(f\"No valid images found in {self.root_path}\")\n",
    "        \n",
    "        # Identify minority classes\n",
    "        if len(self.class_counts) > 0:\n",
    "            median_count = np.median(list(self.class_counts.values()))\n",
    "            self.minority_classes = {cls for cls, count in self.class_counts.items() if count < median_count}\n",
    "        else:\n",
    "            self.minority_classes = set()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.class_specific_aug and self.classes[label] in self.minority_classes:\n",
    "            transform = get_minority_augmentation()\n",
    "        else:\n",
    "            transform = self.transform if self.transform else get_standard_augmentation()\n",
    "        \n",
    "        if transform:\n",
    "            try:\n",
    "                if isinstance(transform, A.Compose):\n",
    "                    augmented = transform(image=image)\n",
    "                    image = augmented['image']\n",
    "                else:\n",
    "                    image = Image.fromarray(image)\n",
    "                    image = transform(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error applying transform: {e}\")\n",
    "                # Fallback to basic tensor conversion\n",
    "                image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def get_standard_augmentation():\n",
    "    \"\"\"Standard augmentation pipeline\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomCrop(224, 224),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 30.0), p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_minority_augmentation():\n",
    "    \"\"\"Enhanced augmentation for minority classes\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomCrop(224, 224),\n",
    "        A.HorizontalFlip(p=0.6),\n",
    "        A.Rotate(limit=20, p=0.7),\n",
    "        A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, p=0.7),\n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.4),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Validation augmentation (minimal)\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca36034",
   "metadata": {},
   "source": [
    "## âš™ï¸ Model Trainer (Train / Validate / Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5506e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Fixed model trainer with better error handling and learning rate scheduling\"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, class_names, device):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.class_names = class_names\n",
    "        self.device = device\n",
    "        self.setup_training()\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def setup_training(self):\n",
    "        \"\"\"Setup training components with proper class weighting\"\"\"\n",
    "        # Calculate class weights\n",
    "        all_labels = []\n",
    "        for batch_idx, (_, labels) in enumerate(self.train_loader):\n",
    "            all_labels.extend(labels.numpy())\n",
    "            if batch_idx > 100:  # Sample first 100 batches for efficiency\n",
    "                break\n",
    "        \n",
    "        unique_classes = np.unique(all_labels)\n",
    "        class_weights = compute_class_weight('balanced', classes=unique_classes, y=all_labels)\n",
    "        \n",
    "        # Create weight tensor\n",
    "        weight_tensor = torch.ones(len(self.class_names))\n",
    "        for i, weight in zip(unique_classes, class_weights):\n",
    "            weight_tensor[i] = weight\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(weight=weight_tensor.to(self.device))\n",
    "        \n",
    "        # Setup optimizer with different learning rates\n",
    "        backbone_params = []\n",
    "        attention_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'backbone' in name:\n",
    "                backbone_params.append(param)\n",
    "            elif 'dilated_block' in name:\n",
    "                attention_params.append(param)\n",
    "            else:\n",
    "                classifier_params.append(param)\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': 1e-5, 'weight_decay': 1e-4},\n",
    "            {'params': attention_params, 'lr': 1e-4, 'weight_decay': 1e-4},\n",
    "            {'params': classifier_params, 'lr': 1e-3, 'weight_decay': 1e-4}\n",
    "        ])\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            self.optimizer, T_0=10, T_mult=2, eta_min=1e-7\n",
    "        )\n",
    "\n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            try:\n",
    "                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, target)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_samples += target.size(0)\n",
    "                correct_predictions += (predicted == target).sum().item()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f'Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader) if len(self.train_loader) > 0 else 0\n",
    "        epoch_acc = 100.0 * correct_predictions / total_samples if total_samples > 0 else 0\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.val_loader:\n",
    "                try:\n",
    "                    data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
    "                    outputs = self.model(data)\n",
    "                    loss = self.criterion(outputs, target)\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_samples += target.size(0)\n",
    "                    correct_predictions += (predicted == target).sum().item()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader) if len(self.val_loader) > 0 else 0\n",
    "        epoch_acc = 100.0 * correct_predictions / total_samples if total_samples > 0 else 0\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def train(self, num_epochs=50, save_best=True, save_path='/content/best_monkeypox_model.pth'):\n",
    "        \"\"\"Main training loop with early stopping\"\"\"\n",
    "        best_val_acc = 0.0\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            print(f'Learning Rate: {self.optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "                \n",
    "                if save_best:\n",
    "                    try:\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': self.model.state_dict(),\n",
    "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                            'val_acc': val_acc,\n",
    "                            'val_loss': val_loss,\n",
    "                            'class_names': self.class_names\n",
    "                        }, save_path)\n",
    "                        print(f\"New best model saved with validation accuracy: {val_acc:.2f}%\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving model: {e}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        return best_val_acc\n",
    "\n",
    "    def evaluate_test(self):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                try:\n",
    "                    data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)\n",
    "                    outputs = self.model(data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    \n",
    "                    y_true.extend(target.cpu().numpy())\n",
    "                    y_pred.extend(predicted.cpu().numpy())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in test batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if len(y_true) == 0:\n",
    "            print(\"No test data processed successfully\")\n",
    "            return 0.0\n",
    "        \n",
    "        test_acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"\\nTest Accuracy: {test_acc * 100:.2f}%\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, target_names=self.class_names, zero_division=0))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.class_names, yticklabels=self.class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return test_acc\n",
    "\n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.train_losses:\n",
    "            print(\"No training history to plot\")\n",
    "            return\n",
    "        \n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        plt.plot(epochs, self.val_losses, 'r-', label='Validation Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.train_accuracies, 'b-', label='Training Accuracy')\n",
    "        plt.plot(epochs, self.val_accuracies, 'r-', label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1eb13",
   "metadata": {},
   "source": [
    "## ðŸ”„ DataLoaders Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9882f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(base_path, classes, batch_size=32, num_workers=2):\n",
    "    \"\"\"Create data loaders with error handling\"\"\"\n",
    "    try:\n",
    "        train_dataset = AdvancedDataset(\n",
    "            base_path / 'train', classes, \n",
    "            transform=get_standard_augmentation(), \n",
    "            class_specific_aug=True\n",
    "        )\n",
    "        val_dataset = AdvancedDataset(\n",
    "            base_path / 'val', classes, \n",
    "            transform=get_validation_augmentation()\n",
    "        )\n",
    "        test_dataset = AdvancedDataset(\n",
    "            base_path / 'test', classes, \n",
    "            transform=get_validation_augmentation()\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "        \n",
    "        # Create weighted sampler for balanced training\n",
    "        if len(train_dataset) > 0:\n",
    "            class_counts = train_dataset.class_counts\n",
    "            labels = [label for _, label in train_dataset.samples]\n",
    "            \n",
    "            if len(set(labels)) > 1:  # Only if we have multiple classes\n",
    "                class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "                sample_weights = [class_weights[label] for label in labels]\n",
    "                sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "            else:\n",
    "                sampler = None\n",
    "        else:\n",
    "            sampler = None\n",
    "        \n",
    "        # Adjust num_workers based on availability\n",
    "        if num_workers > 0 and not torch.cuda.is_available():\n",
    "            num_workers = min(num_workers, os.cpu_count())\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            sampler=sampler,\n",
    "            shuffle=(sampler is None),\n",
    "            num_workers=num_workers, \n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=num_workers, \n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader, train_dataset.class_counts\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data loaders: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f2c62",
   "metadata": {},
   "source": [
    "## ðŸš€ Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31952339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(zip_path):\n",
    "    \"\"\"Main execution function with comprehensive error handling\"\"\"\n",
    "    try:\n",
    "        # Device setup\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        print(\"=== Setting up Dataset ===\")\n",
    "        dataset_manager = DatasetManager(zip_path)\n",
    "        df_counts = dataset_manager.extract_and_setup()\n",
    "        \n",
    "        classes = dataset_manager.classes\n",
    "        base_path = dataset_manager.base_path\n",
    "        print(f\"Final classes: {classes}\")\n",
    "        \n",
    "        print(\"\\n=== Creating Data Loaders ===\")\n",
    "        train_loader, val_loader, test_loader, class_counts = create_data_loaders(\n",
    "            base_path, classes, batch_size=16, num_workers=2  # Reduced batch size\n",
    "        )\n",
    "        \n",
    "        print(\"Class distribution in training set:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            print(f\"{cls}: {count}\")\n",
    "        \n",
    "        print(\"\\n=== Initializing Model ===\")\n",
    "        model = DilatedAttentionResNet(num_classes=len(classes), pretrained=True)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        print(\"\\n=== Setting up Trainer ===\")\n",
    "        trainer = ModelTrainer(model, train_loader, val_loader, test_loader, classes, device)\n",
    "        \n",
    "        print(\"\\n=== Starting Training ===\")\n",
    "        best_val_acc = trainer.train(num_epochs=30, save_best=True)  # Reduced epochs for testing\n",
    "        \n",
    "        print(\"\\n=== Plotting Training History ===\")\n",
    "        trainer.plot_training_history()\n",
    "        \n",
    "        print(\"\\n=== Evaluating on Test Set ===\")\n",
    "        # Load best model if available\n",
    "        save_path = '/content/best_monkeypox_model.pth'\n",
    "        if os.path.exists(save_path):\n",
    "            try:\n",
    "                checkpoint = torch.load(save_path, map_location=device)\n",
    "                trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                print(\"Loaded best model for evaluation\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading best model: {e}, using current model\")\n",
    "        \n",
    "        test_acc = trainer.evaluate_test()\n",
    "        \n",
    "        print(\"\\n=== Training Complete ===\")\n",
    "        print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "        print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "        \n",
    "        return trainer, best_val_acc, test_acc\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, 0.0, 0.0\n",
    "\n",
    "# Example usage - Update these paths according to your setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e4281",
   "metadata": {},
   "source": [
    "### â–¶ï¸ Run Training (Colab Entry Point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set this path to your dataset zip file in Colab or Drive\n",
    "zip_path = '/content/drive/MyDrive/monkeypox_dataset.zip'\n",
    "\n",
    "# Upload manually if needed\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# zip_path = '/content/monkeypox_dataset.zip'\n",
    "\n",
    "# Run training\n",
    "if os.path.exists(zip_path):\n",
    "    trainer, best_val_acc, test_acc = main(zip_path)\n",
    "else:\n",
    "    print(\"Dataset file not found. Please upload and set correct path.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}